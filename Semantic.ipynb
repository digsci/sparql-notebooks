{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Web Technology - Data Science Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some simple formatting defaults for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook works through the process of extracting data from an Excel spreadsheet, converting the data using the Resource Description Framework (RDF), loading RDF data into a database, semantically enriching the data using ontology languages, and finally demonstrating some flexible querying mechanisms.\n",
    "\n",
    "### Technologies used in this notebook\n",
    "\n",
    "Category | Technology | Link\n",
    "-------- | ---------- | ----\n",
    "User Interface | Jupyter | [Jupyter](http://jupyter.org) <br />\n",
    "Raw Data | Excel Spreadsheet | [Excel Description](https://en.wikipedia.org/wiki/Microsoft_Excel) <br />\n",
    "Database | Virtuoso Open-Source | [Virtuoso GitHub](https://github.com/openlink/virtuoso-opensource) <br />\n",
    "Resource Description | RDF | [RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework) <br />\n",
    "Ontology Description | RDFS / OWL | [RDFS](https://en.wikipedia.org/wiki/RDF_Schema) / [OWL](https://en.wikipedia.org/wiki/Web_Ontology_Language) <br />\n",
    "RDF Conversion Utility | csv2rdf | [RDFLib GitHub](https://github.com/RDFLib/rdflib/blob/master/rdflib/tools/csv2rdf.py) <br />\n",
    "Query Language | SPARQL | [SPARQL](https://en.wikipedia.org/wiki/SPARQL) <br />\n",
    "Programming Language | Python3 | [Python3](https://www.python.org) <br />\n",
    "Data Handling | Pandas | [Pandas](http://pandas.pydata.org) <br />\n",
    "SPARQL Wrapper | SPARQLWrapper | [SPARQLWrapper GitHub](https://github.com/RDFLib/sparqlwrapper) <br />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion and uploading of data into the database / triple store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the filename of the Excel spreadsheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/communities/*.xlsm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pandas to load the spreadsheet and list worksheet names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crdata = pd.ExcelFile(\"Example.xlsm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crdata.sheet_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two interesting worksheets are 'Gauge' and 'Property' that give a lot of data about gauging stations and properties respectively. We'll use these two worksheets in the rest of the notebook.\n",
    "\n",
    "Load the gauge worksheet into a pandas dataframe and get overview of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gauge_df = crdata.parse('Gauge', header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the property worksheet into a pandas dataframe and get overview of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "property_df = crdata.parse('Property')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the gauge and property worksheets to csv files for subsequent RDF processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gauge_df.to_csv('data/communities/gauge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "property_df.to_csv('data/communities/property.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now convert the csv files to triple format using the csv2rdf utility. RDF namespaces for subject base names and property basenames. The resulting RDF files are in [Turtle](https://www.w3.org/TR/turtle/) syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# http://ensembleprojects.org/ds/ns/floodrisk/gauge#\n",
    "# http://ensembleprojects.org/ds/ns/floodrisk/gauge_data#\n",
    "\n",
    "!python data/communities/csv2rdf.py -b http://ensembleprojects.org/ds/ns/floodrisk/gauge# -p http://ensembleprojects.org/ds/ns/floodrisk/gauge_data# -o data/communities/gauge_ds.ttl data/communities/gauge.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a quick look at the resulting triples and check the namespaces have been generated correctly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/communities/gauge_ds.ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate the property triples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://ensembleprojects.org/ds/ns/floodrisk/property#\n",
    "# http://ensembleprojects.org/ds/ns/floodrisk/property_data#\n",
    "\n",
    "!python data/communities/csv2rdf.py -b http://ensembleprojects.org/ds/ns/floodrisk/property# -p http://ensembleprojects.org/ds/ns/floodrisk/property_data# -o data/communities/property_ds.ttl data/communities/property.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/communities/*.ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the generated files into the Virtuoso database. Virtuoso uses trusted directories for uploading of data, so the ttl files care copied there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy ttl files to allowed Virtuoso import directory\n",
    "!cp data/communities/gauge_ds.ttl /usr/local/Cellar/virtuoso/7.2.4.2/share/virtuoso/vad/\n",
    "!cp data/communities/property_ds.ttl /usr/local/Cellar/virtuoso/7.2.4.2/share/virtuoso/vad/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the files into Virtuoso using the isql interface and two small batch files. The triples are loaded into two separate named graphs: (i) <http://ensembleprojects.org/ds/floodrisk/gauge> and (ii) <http://ensembleprojects.org/ds/floodrisk/gauge>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/communities/load_gauge_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/communities/load_property_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gauge ttl files into Virtuoso named graphs\n",
    "!isql localhost dba dba data/communities/load_gauge_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load property ttl files into Virtuoso named graphs\n",
    "!isql localhost dba dba data/communities/load_property_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying of Semantic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the raw data has been converted into triple form and uploaded into the Virtuoso triple store, we can query it using the SPARQL language. In this case the SPARQL query is embedded in Python using the SPARQLWrapper package. We have created two separate named graphs so we can query across one or both of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# Create the SPARQL query as a string. To illustrate querying, we can query across \n",
    "# both named graphs using the 'FROM' clause or, as in this case, simply comment out\n",
    "# one of the named graphs.\n",
    "\n",
    "sparql_query = \"\"\"\n",
    "SELECT ?subject ?predicate ?object\n",
    "FROM <http://ensembleprojects.org/ds/floodrisk/gauge>\n",
    "#FROM <http://ensembleprojects.org/ds/floodrisk/property>\n",
    "WHERE {\n",
    "  ?subject ?predicate ?object\n",
    "}\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "# Virtuoso SPARQL endpoint\n",
    "sparql_endpoint = \"http://localhost:0/sparql\"\n",
    "sparql = SPARQLWrapper(sparql_endpoint)\n",
    "\n",
    "# Return results in JSON format\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "sparql.setQuery(sparql_query)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result[\"subject\"][\"value\"], result[\"predicate\"][\"value\"], result[\"object\"][\"value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantically enriching the data using an ontology "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The existing property dataset has different damage estimations on a per building basis for a number of different return periods. A simple exmaple of semantically enriching the data is to model these different return periods as a class hierachy; we can then query either specific return periods or all return periods.\n",
    "\n",
    "The first step is to find the RDF properties that relate to the different damage estimations. To do this, we use a regular expression based filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "sparql_query = \"\"\"\n",
    "SELECT DISTINCT ?property\n",
    "FROM <http://ensembleprojects.org/ds/floodrisk/property>\n",
    "WHERE {\n",
    "  ?s ?property ?o .\n",
    "  FILTER regex(?property,'existingdamage','i')\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "sparql_endpoint = \"http://localhost:0/sparql\"\n",
    "sparql = SPARQLWrapper(sparql_endpoint)\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "sparql.setQuery(sparql_query)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result['property']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look like the correct RDF properites for damage estimations. We can model these as a property hierachy with a top-level 'Q' return period and subsequent sub-properties for individual return periods.\n",
    "\n",
    "We do this using the RDFS ontology languge. We declare 'Q' as a rdf:Property type and then declare the specific return period properties as a rdfs:subPropertyOf of 'Q'. In Turtle syntax:\n",
    "\n",
    "Q a rdf:Property . <br />\n",
    "q2_existingDamageMean rdfs:subPropertyOf Q . <br />\n",
    "q5_existingDamageMean rdfs:subPropertyOf Q . <br />\n",
    "... <br />\n",
    "\n",
    "\n",
    "The SPARQL code below generates the triples representing this ontology and inserts them into the <http://ensembleprojects.org/ds/floodrisk/property> named graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "sparql_query = \"\"\"\n",
    "PREFIX g:       <http://ensembleprojects.org/ns/floodrisk/gauge#>\n",
    "PREFIX gd:      <http://ensembleprojects.org/ns/floodrisk/gauge_data#>\n",
    "PREFIX p:       <http://ensembleprojects.org/ns/floodrisk/property#>\n",
    "PREFIX pd:      <http://ensembleprojects.org/ns/floodrisk/property_data#>\n",
    "PREFIX powl:    <http://ensembleprojects.org/owl/propertymodel#>\n",
    "PREFIX rdf:     <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdfs:    <http://www.w3.org/2000/01/rdf-schema#> \n",
    "\n",
    "INSERT {\n",
    "  powl:Q a rdf:Property . \n",
    "  ?p rdfs:subPropertyOf powl:Q \n",
    "}\n",
    "FROM <http://ensembleprojects.org/ds/floodrisk/property>\n",
    "WHERE {\n",
    "  ?s ?p ?o .\n",
    "  FILTER regex(?p,'existingdamage','i')\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "sparql_endpoint = \"http://localhost:0/sparql\"\n",
    "sparql = SPARQLWrapper(sparql_endpoint)\n",
    "# As we're updating the triple store, we need to use the 'POST' method\n",
    "sparql.setMethod('POST')\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "sparql.setQuery(sparql_query)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the ontology triples are in the database, we need to tell Virtuoso to generate new triples using its inferencing engine. This is done through the Virtuoso isql interface:\n",
    "\n",
    "$ isql <br />\n",
    "SQL> rdfs_rule_set('http://ensembleprojects.org/ds/floodrisk/property',  'http://ensembleprojects.org/ds/floodrisk/property'); <br />\n",
    "SQL>exit; <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Virtuoso, we use the 'DEFINE'statement to give a custom inferencing context. We can then query damage estimations for all return periods using the generic 'Q' return period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "sparql_query = \"\"\"\n",
    "DEFINE input:inference <http://ensembleprojects.org/ds/floodrisk/property>\n",
    "PREFIX p:       <http://ensembleprojects.org/ds/ns/floodrisk/property#>\n",
    "PREFIX powl:    <http://ensembleprojects.org/owl/propertymodel#>\n",
    "\n",
    "SELECT *\n",
    "FROM <http://ensembleprojects.org/ds/floodrisk/property>\n",
    "WHERE {\n",
    "  p:0 powl:Q ?value .\n",
    "}\n",
    "\"\"\"\n",
    "sparql_endpoint = \"http://localhost:0/sparql\"\n",
    "sparql = SPARQLWrapper(sparql_endpoint)\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "sparql.setQuery(sparql_query)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result[\"value\"]['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can get all the return period damage estimations using the above method, in general we want to know both the values and the associated return periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "sparql_query = \"\"\"\n",
    "DEFINE input:inference <http://ensembleprojects.org/ds/floodrisk/property>\n",
    "PREFIX p:       <http://ensembleprojects.org/ds/ns/floodrisk/property#>\n",
    "PREFIX pd:      <http://ensembleprojects.org/ds/ns/floodrisk/property_data#>\n",
    "PREFIX powl:    <http://ensembleprojects.org/owl/propertymodel#>\n",
    "PREFIX rdf:     <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdfs:    <http://www.w3.org/2000/01/rdf-schema#> \n",
    "\n",
    "SELECT ?returnPeriod ?value\n",
    "FROM <http://ensembleprojects.org/ds/floodrisk/property>\n",
    "WHERE {\n",
    "  p:0 ?returnPeriod ?value .\n",
    "  ?returnPeriod rdfs:subPropertyOf powl:Q\n",
    "}\n",
    "\"\"\"\n",
    "sparql_endpoint = \"http://localhost:0/sparql\"\n",
    "sparql = SPARQLWrapper(sparql_endpoint)\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "sparql.setQuery(sparql_query)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "returnPeriods = []\n",
    "values = []\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    returnPeriod = int(result['returnPeriod']['value'].split('#')[1].split('_')[0].split('q')[1])\n",
    "    returnPeriods.append(returnPeriod)\n",
    "    values.append(result['value']['value'])\n",
    "\n",
    "vals = zip(returnPeriods, values)\n",
    "sorted_vals = sorted(vals)\n",
    "returnPeriods = [val[0] for val in sorted_vals]\n",
    "values = [val[1] for val in sorted_vals]\n",
    "\n",
    "print('Period\\t\\tDamage Estimation')\n",
    "for i in range(len(returnPeriods)):\n",
    "    print(returnPeriods[i], '\\t\\t', values[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
